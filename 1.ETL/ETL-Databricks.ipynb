{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook # 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de datos estática"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Acceso al sitio*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL de Fuente: TLC Trip Record Data.\n",
    "tlc_url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "#Solicitar URL y obtener objeto de respuesta.\n",
    "tlc_respuesta = requests.get(tlc_url)\n",
    "\n",
    "#Analizar el texto obtenido.\n",
    "tlc_sopa = BeautifulSoup(tlc_respuesta.text, 'html.parser')\n",
    "\n",
    "#Encontrar todos los hipervínculos presentes en la página web.\n",
    "tlc_vinculos = tlc_sopa.find_all('a')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descarga inicial.\n",
    "#Yellow Taxi Trip Records.\n",
    "\n",
    "años = ['2022']\n",
    "\n",
    "i = 0\n",
    "d_amarillo = {}\n",
    "dh_amarillo = {}\n",
    "\n",
    "for vinculo in tlc_vinculos:\n",
    "    if(\n",
    "        ('yellow' in vinculo.get('href', [])) and\n",
    "        ('.parquet' in vinculo.get('href', []))\n",
    "    ):\n",
    "        for año in años:\n",
    "            if (año in vinculo.get('href', [])):\n",
    "                i += 1\n",
    "                hipervinculo = vinculo.get('href', [])\n",
    "                mes = hipervinculo[69]+hipervinculo[70]\n",
    "                print('Elemento', i, 'leído.')\n",
    "                #Leer el vínculo y asignarlo a un diccionario.\n",
    "                d_amarillo[\"yellow_{0}_{1}\".format(año, mes)] = \"yellow_{0}_{1} = pd.read_parquet('{2}')\".format(año, mes, hipervinculo)\n",
    "                print('Elemento', i, 'escrito en diccionario.')\n",
    "\n",
    "print('Todos los elementos se escribieron en el diccionario amarillo.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Acceso a Blob Storage* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos de acceso\n",
    "storage_account_name = \"pfcrudo\"\n",
    "storage_account_key = \"ASY9nV2+pddQN2Y8AUZef5m4JtdidWOa5RW+fNIkj2HE60F5s95ueJraFC2/Jv1KulbyL5hwcYk6+AStCn7cjw==\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable de configuración Spark\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga de todo lo contenido en el diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permitir a Apache Arrow convertir Pandas a pySpark DF:\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"yellowtaxis\"\n",
    "blobstorage = f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net\"\n",
    "\n",
    "for variable in d_amarillo.values():\n",
    "    exec(variable)\n",
    "\n",
    "    for clave, valor in d_amarillo.items():\n",
    "        if variable == valor:\n",
    "\n",
    "            #Código para exportar al almacenamiento previo (antes de transformar)\n",
    "            dataframe = spark.createDataFrame(vars()[clave])\n",
    "            \n",
    "            dataframe.write.format(\"parquet\").save(f\"wasbs://yellowtaxis@pfcrudo.blob.core.windows.net/{clave}.parquet\")\n",
    "\n",
    "            print(clave, \"exportado a .parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportar archivos pickles de los diccionarios de referencia al directorio del almacenamiento conveniente.\n",
    "\n",
    "directorio_diccionarios = \"/dbfs/d_amarillo.pickle\"\n",
    "\n",
    "amarillo = open(directorio_diccionarios, 'wb')\n",
    "pickle.dump(d_amarillo, amarillo)\n",
    "amarillo.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de datos automática"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceso al sitio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL de Fuente: TLC Trip Record Data\n",
    "tlc_url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "#Solicitar URL y obtener objeto de respuesta.\n",
    "tlc_respuesta = requests.get(tlc_url)\n",
    "\n",
    "#Analizar el texto obtenido.\n",
    "tlc_sopa = BeautifulSoup(tlc_respuesta.text, 'html.parser')\n",
    "\n",
    "#Encontrar todos los hipervínculos presentes en la página web.\n",
    "tlc_vinculos = tlc_sopa.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leer diccionarios de referencia.\n",
    "directorio_diccionarios = \"/dbfs/d_amarillo.pickle\"\n",
    "d_amarillo = pd.read_pickle(directorio_diccionarios)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descarga de nuevos archivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acceso a Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos de acceso\n",
    "storage_account_name = \"pfcrudo\"\n",
    "storage_account_key = \"ASY9nV2+pddQN2Y8AUZef5m4JtdidWOa5RW+fNIkj2HE60F5s95ueJraFC2/Jv1KulbyL5hwcYk6+AStCn7cjw==\"\n",
    "container = \"yellowtaxis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable de configuración Spark\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permitir a Apache Arrow convertir Pandas a pySpark DF:\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener el año en curso.\n",
    "date = datetime.date.today()\n",
    "año = '{0}'.format(date.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparar con diccionario actual. Agregar (y exportar) sólo nueva variable.\n",
    "for vinculo in tlc_vinculos:\n",
    "    if(\n",
    "        'yellow' in vinculo.get('href', []) and\n",
    "        '.parquet' in vinculo.get('href', [])\n",
    "    ):\n",
    "        if (\n",
    "            año in vinculo.get('href', [])\n",
    "        ):\n",
    "            hipervinculo = vinculo.get('href', [])\n",
    "            mes = hipervinculo[69]+hipervinculo[70]\n",
    "\n",
    "            #Leer el vínculo y compararlo con las otras entradas del diccionario.\n",
    "\n",
    "            for clave, valor in d_amarillo.items():\n",
    "                valor_año = valor[7]+valor[8]+valor[9]+valor[10]\n",
    "                valor_mes = valor[13]+valor[14]\n",
    "                \n",
    "                if (\n",
    "                    (año == valor_año) and\n",
    "                    (mes == valor_mes)\n",
    "                ):\n",
    "                    equal = True\n",
    "                    break\n",
    "                else:\n",
    "                    equal = False\n",
    "\n",
    "            if (equal == False):\n",
    "                #Agregar entrada nueva al diccionario.\n",
    "                d_amarillo[\"yellow_{0}_{1}\".format(año, mes)] = \"yellow_{0}_{1} = pd.read_parquet('{2}')\".format(año, mes, hipervinculo)\n",
    "\n",
    "                #Exportar diccionario actualizado a almacenamiento conveniente.\n",
    "\n",
    "                amarillo = open(directorio_diccionarios, 'wb')\n",
    "                pickle.dump(d_amarillo, amarillo)\n",
    "                amarillo.close()\n",
    "\n",
    "                #Exportar nuevo archivo parquet (tabla) a almacenamiento previo (raw).\n",
    "\n",
    "                #Ejecutar el comando\n",
    "                exec(d_amarillo[\"yellow_{0}_{1}\".format(año, mes)])\n",
    "\n",
    "                #Pasar dataframe a parquet en la ubicación\n",
    "\n",
    "                dataframe = spark.createDataFrame(vars()[list(d_amarillo)[-1]])\n",
    "\n",
    "                dataframe.write.format(\"parquet\").save(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/yellow_{año}_{mes}.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook #3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tubería con Auto Loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Acceso a Blob Storage*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_account_name = \"pfcrudo\"\n",
    "storage_account_key = \"ASY9nV2+pddQN2Y8AUZef5m4JtdidWOa5RW+fNIkj2HE60F5s95ueJraFC2/Jv1KulbyL5hwcYk6+AStCn7cjw==\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listado de archivos dentro del container\n",
    "%fs ls wasbs://yellowtaxis@pfcrudo.blob.core.windows.net/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer archivo y obtener el esquema general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"yellowtaxis\"\n",
    "dataframe = spark.read.parquet(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/yellow_2021_12.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probar si se ve el esquema\n",
    "dataframe.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probar si se ve el tipo de objeto\n",
    "type(dataframe.schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establecer el esquema deseado (para reducir memoria):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ByteType, TimestampType, ShortType, StringType, FloatType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportar el esquema del archivo a JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar el schema a un archivo JSON en el almacenamiento de Databricks.\n",
    "import json\n",
    "with open(\"/dbfs/taxischema.json\", \"w\") as f: \n",
    "    json.dump(dataframe.schema.jsonValue(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probar si se lee el esquema desde el archivo JSON.\n",
    "with open(\"/dbfs/taxischema.json\") as sch_file:\n",
    "    dataset_schema = StructType.fromJson(json.load(sch_file))\n",
    "    #print(dataset_schema.simpleString())\n",
    "dataset_schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_in = (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.useNotifications\", False)\n",
    "                .option(\"cloudFiles.format\", \"parquet\")\n",
    "                .schema(dataset_schema)\n",
    "                .load(\"wasbs://yellowtaxis@pfcrudo.blob.core.windows.net/\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, mode, abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar columnas indeseadas y rellenar nulos.\n",
    "df = (\n",
    "    stream_in\n",
    "    .drop(\"VendorID\", \"RatecodeID\", \"store_and_fwd_flag\")\n",
    "    .na.fill(value = 0, subset = [\"fare_amount\", \"extra\", \"mta_tax\", \"tolls_amount\", \"improvement_surcharge\", \"congestion_surcharge\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignorar filas con ID de ubicación desconocida (\"PULocationID\", \"DOLocationID\", 264, 265).\n",
    "df = df.filter(\n",
    "    (col(\"PULocationID\") != 264) & \n",
    "    (col(\"PULocationID\") != 265) & \n",
    "    (col(\"DOLocationID\") != 264) & \n",
    "    (col(\"DOLocationID\") != 265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar filas con distancia == 0 o > 250\n",
    "df=df.filter(col(\"trip_distance\")!=0)\n",
    "\n",
    "df=df.filter(col(\"trip_distance\")<250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usar valores absolutos para columnas numéricas.\n",
    "df = df.withColumn(\"passenger_count\", abs(col(\"passenger_count\")))\n",
    "df = df.withColumn(\"trip_distance\", abs(col(\"trip_distance\")))\n",
    "df = df.withColumn(\"fare_amount\", abs(col(\"fare_amount\")))\n",
    "df = df.withColumn(\"total_amount\", abs(col(\"total_amount\")))\n",
    "df = df.withColumn(\"extra\", abs(col(\"extra\")))\n",
    "df = df.withColumn(\"mta_tax\", abs(col(\"mta_tax\")))\n",
    "df = df.withColumn(\"tip_amount\", abs(col(\"tip_amount\")))\n",
    "df = df.withColumn(\"tolls_amount\", abs(col(\"tolls_amount\")))\n",
    "df = df.withColumn(\"improvement_surcharge\", abs(col(\"improvement_surcharge\")))\n",
    "df = df.withColumn(\"congestion_surcharge\", abs(col(\"congestion_surcharge\")))\n",
    "df = df.withColumn(\"airport_fee\", abs(col(\"airport_fee\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga a SQL Database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexión a SQL Database:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def output_sqldb(batch_df, batch_id):\n",
    "    #Unas últimas transformaciones.\n",
    "    right_year = batch_df.select(mode(year('tpep_pickup_datetime'))).head()[0]\n",
    "    batch_df = batch_df.filter(year(\"tpep_pickup_datetime\") == right_year)\n",
    "\n",
    "    #Propiedades para la conexión.\n",
    "    jdbcHostname = \"pfservidor.database.windows.net\"\n",
    "    jdbcPort = 1433\n",
    "    jdbcDatabase = \"yellowtaxis\"\n",
    "    jdbcUsername = \"ivnng\"\n",
    "    jdbcPassword = \"henryPF2023\"\n",
    "    jdbcDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "    jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase};user={jdbcUsername};password={jdbcPassword}\"\n",
    "\n",
    "    dbtable = \"trips\"\n",
    "\n",
    "    #Convertir de PySpark DataFrame a Azure DW...\n",
    "    batch_df.write.format(\"jdbc\").mode(\"append\").option(\"url\", jdbcUrl).option(\"dbtable\", dbtable).save()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_out = (df.writeStream\n",
    "                .trigger(once = True)\n",
    "                .foreachBatch(output_sqldb)\n",
    "                .option(\"checkpointLocation\", \"wasbs://checkpoint@pfcrudo.blob.core.windows.net/\")\n",
    "                .start())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
